{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e64c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing OPEN AI GYM dependancies \n",
    "!pip install gym_super_mario_bros==7.3.0 nes_py\n",
    "\n",
    "!pip install stable-baselines3[extra]\n",
    "\n",
    "!pip install tensorflow-tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the environment and agent actions.\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e803d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    #Convert the image observation from RGB to gray scale.\n",
    "    \n",
    "\n",
    "    def __init__(self, env: gym.Env, keep_dim: bool = False):\n",
    "        #Convert the image observation from RGB to gray scale.\n",
    "        #Args:\n",
    "            #env (Env): The environment\n",
    "            #keep_dim (bool): If `True`, a singleton dimension will be added\n",
    "            \n",
    "        super().__init__(env)\n",
    "        self.keep_dim = keep_dim\n",
    "\n",
    "        assert (\n",
    "            len(env.observation_space.shape) == 3\n",
    "            and env.observation_space.shape[-1] == 3\n",
    "        )\n",
    "\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        if self.keep_dim:\n",
    "            self.observation_space = Box(\n",
    "                low=0, high=255, shape=(obs_shape[0], obs_shape[1], 1), dtype=np.uint8\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = Box(\n",
    "                low=0, high=255, shape=obs_shape, dtype=np.uint8\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d6af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing the environment/agent with sample action\n",
    "done = True\n",
    "\n",
    "for step in range(100000):\n",
    "    if done:\n",
    "        env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bb3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing needed Wrappers \n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1becb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising and formating the environment for the CNN\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = GrayScaleObservation(env, keep_dim = True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order = \"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b20067",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state, reward, done, info = env.step([env.action_space.sample()])\n",
    "state.shape\n",
    "plt.imshow(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26789ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Proximal Policy Optimization algorithm \n",
    "import os \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23cb60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Callback class to save the model \n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d502e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalising the callback path\n",
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR=\"./logs/\"\n",
    "callback = TrainAndLoggingCallback(check_freq = 1000, save_path = CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e51013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the model with Proximal Policy Optimization\n",
    "model = PPO(\"CnnPolicy\",env,verbose = 1, tensorboard_log=LOG_DIR, learning_rate = 0.000001, n_steps = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d435f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the learning process. \n",
    "model.learn(total_timesteps = 100000, callback = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ce386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading saved model \n",
    "model=PPO.load(\"./train/best_model_14000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment \n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _state = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
